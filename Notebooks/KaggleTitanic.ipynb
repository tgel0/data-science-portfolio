{
  "cells": [
    {
      "metadata": {
        "_uuid": "0b155d82edceca2a7c3d6f422f16a9491de9daa4"
      },
      "cell_type": "markdown",
      "source": "# Titanic Prediction Competition\n# Intro\nThis notebook demonstrates a few approaches to the Titanic prediction competition including using a gender-only model (no machine learning at all!) as well as some machine learning algorithms using random forest classifiers. The last model was powered by the AutoML tool [TPOT](https://epistasislab.github.io/tpot/) which resulted in a **public score of 0.81818 (top 4%).**\n\nHere is the official competition description from [Kaggle](https://www.kaggle.com/c/titanic):\n\n> ### Competition Description\n> The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n> \n> One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n> \n> In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\n# Data Loading"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# import libraries for data analysis\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# list data files that are connected to the kernel\nimport os\nos.listdir('../input/')",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6fa22e5a88b65ff1d98b930f36b6e3c0d425fa6f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# read the train.csv file into a datframe\ndf_train = pd.read_csv('../input/train.csv')\nprint('Shape: ', df_train.shape)\ndf_train.head()",
      "execution_count": 81,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e46006c194e31886a79c4b73cdc407ca730d88fd",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# read the test.csv file into a datframe\ndf_test = pd.read_csv('../input/test.csv')\nprint('Shape: ', df_test.shape)\ndf_test.head()",
      "execution_count": 82,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8437b23557325d4d23a5890da900c531b5450415",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# create df_full by merging both train and test data\ndf_full = df_train.append(df_test, sort=False)\nprint('Shape: ', df_full.shape)",
      "execution_count": 91,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "41168d1e7f818abf19e4843ea27523c9d634dc06"
      },
      "cell_type": "markdown",
      "source": "# Exploratory Data Analysis"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e2fb5105b8e4d4867ab5f86dfe73729bcc948461",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df_train.info()",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3083e91f6e0bf2caac544a2aae89fd3507b72a93",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df_test.info()",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1e5147cf9aee7c1f17fbab2046118de355807c93",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# import LabelEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n# create function EDA_helper to do which is doing 3 things: binning, encoding of the feature and calculating the impact on the target feature\ndef EDA_helper(feature, bin_number=5, train_size=891):\n    '''function creates a new column as 'old feature name_bin' and bins the values (only if the number of unique values is more than 10). After that it calculates the sum, count and mean of the feature values'''    \n    # calculate number of unique values for the feature\n    unique_features = len(list(df_full[feature].unique()))\n    # if there are more than 10 unique values\n    if unique_features > 10:\n        print('Number of unique features is %d, starting to bin...' % unique_features)\n        # create a new column for the bins\n        df_full[feature + '_bin'] = pd.qcut(df_full[feature], bin_number)\n        # assign the bins to the train and test dataframe\n        df_train[feature + '_bin'] = df_full[feature + '_bin'][:train_size]\n        df_test[feature + '_bin'] = df_full[feature + '_bin'][train_size:]\n        # define LabelEncoder instance \n        label = LabelEncoder()\n        # fit and transform the data\n        df_full[feature + '_bin_code'] = label.fit_transform(df_full[feature + '_bin'].astype(str))\n        # assign the encoded bins to the train and test dataframe\n        df_train[feature + '_bin_code'] = df_full[feature + '_bin_code'][:train_size]\n        df_test[feature + '_bin_code'] = df_full[feature + '_bin_code'][train_size:]\n        print('Binning successful, calculating impact...')\n        # calculate the statistics\n        impact = df_full[[feature + '_bin', 'Survived']].groupby([feature + '_bin']).agg(['sum','count','mean']).rename(columns={'sum':'Yes','count':'Total','mean':'In %'})\n    else:\n        print('Number of unique features is %d, binning not needed. Calculating impact...' % unique_features)\n        # define LabelEncoder instance \n        label = LabelEncoder()\n        # fit and transform the data\n        df_full[feature + '_code'] = label.fit_transform(df_full[feature])\n        # assign the encoded bins to the train and test dataframe\n        df_train[feature + '_code'] = df_full[feature + '_code'][:train_size]\n        df_test[feature + '_code'] = df_full[feature + '_code'][train_size:]\n        # calculate the statistics for not binned features\n        impact = df_full[[feature, 'Survived']].groupby([feature]).agg(['sum','count','mean']).rename(columns={'sum':'Yes','count':'Total','mean':'In %'})\n    return impact",
      "execution_count": 99,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3c383b5c20373318c744cbf9db6e2a910fc1727a"
      },
      "cell_type": "markdown",
      "source": "### PassengerId"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b4b2b237580854502ac13fae45256672dc9983c7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# using the function on the 'PassengerId' column\nEDA_helper('PassengerId')",
      "execution_count": 105,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "af5b9cc90a534924d0e85f003a731c652767db52"
      },
      "cell_type": "markdown",
      "source": "### Survived"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "625572f25baf00d621e818f9e1e0c03c982a1ebc",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# unique value counts in 'Survived' column\ndf_train['Survived'].value_counts()",
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "17450b80f8df5b765c1d57f91e76e1c66fb25a0b"
      },
      "cell_type": "markdown",
      "source": "### Pclass"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72d85c70d5bf370bf01528a8cb2552dd69d291c4",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "EDA_helper('Pclass')",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bcaef52bef0ca27cc693fa6e6cb49cd15e6bf68a"
      },
      "cell_type": "markdown",
      "source": "### Name"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b3170286a737a8bb340c7e7aabf47562494d9768",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# extract the title from the 'Name' column\nfor name in df_full['Name']:\n    df_full['Title'] = df_full['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\n\n# check how the different titles are distributed by gender\nprint(pd.crosstab(df_full['Title'], df_full['Sex']))",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4006aeb81c035cca456801cdd936ab6ab205cde7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# categorize titles\nfor title in df_full['Title']:\n    df_full['Title'] = df_full['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n                                                'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare'\n                                               )\n    df_full['Title'] = df_full['Title'].replace('Mlle', 'Miss')\n    df_full['Title'] = df_full['Title'].replace('Ms', 'Miss')\n    df_full['Title'] = df_full['Title'].replace('Mme', 'Mrs')\n    \n# use the EDA_helper function\nEDA_helper('Title')",
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "54f045752b91d7bf096dff8eb4c282df37946e91"
      },
      "cell_type": "markdown",
      "source": "### Sex"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f95943493e31ae7a2439195c8880ff486eb2909a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "EDA_helper('Sex')",
      "execution_count": 106,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ed83baf98e86a3d7808c330e130dc24a786088c9"
      },
      "cell_type": "markdown",
      "source": "### Age"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "130fbc7c2a8bd2c078ef31a6d11e0cb6d0dbe80f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# fill the missing age info with median from the full dataset\nfor age in df_full['Age']:\n    df_full['Age'].fillna(df_full['Age'].median(), inplace=True)\n\n# using the EDA_helper function and setting number of bins to 4\nEDA_helper('Age', 4)",
      "execution_count": 57,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d2354b7f46da4b23173682f30d81020efd2b5439"
      },
      "cell_type": "markdown",
      "source": "### SibSp & Parch"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e1da456d212deabc36127bb320b5717973cacd64",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# combine both columns as 'Family size'\ndf_full['Family Size'] = df_full['SibSp'] + df_full['Parch']\n\nEDA_helper('Family Size')",
      "execution_count": 113,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dcab098cc0802533ee4818c0abcc637fd5a93575"
      },
      "cell_type": "markdown",
      "source": "### Ticket"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9adc3701973729964c570a4e61a581878e7b486d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# import library\nimport re\n\n# remove non-digits from the ticket and change to numeric type\nfor ticket in df_full['Ticket']:\n    df_full['Ticket'] = df_full['Ticket'].apply(lambda x: x if x.isdigit() else re.sub('\\D','', x))\n\n# changing the type to numeric\ndf_full['Ticket'] = df_full['Ticket'].apply(pd.to_numeric)\n    \nEDA_helper('Ticket')",
      "execution_count": 100,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c06440108efde44779cd7afe7b0f0ef49f7eca30"
      },
      "cell_type": "markdown",
      "source": "### Fare"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c34e31a5abc780dfb89e82475b9a1f312de0763b",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# fill the missing fare info with median fare from the full dataset\nfor fare in df_full['Fare']:\n    df_full['Fare'].fillna(df_full['Fare'].median(), inplace=True)\n\nEDA_helper('Fare')",
      "execution_count": 124,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c78bb95ea30df788add2433582ee83daae94ffb3"
      },
      "cell_type": "markdown",
      "source": "### Cabin"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "59ee0db1ac40f248e9320a7e077e1f76bdbd51ee",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# fill the missing info with string 'N' and extract the first letter as new column 'N' for the full dataset\nfor cabin in df_full['Cabin']:\n    df_full['Cabin'].fillna('N', inplace=True)\n    df_full['Deck'] = df_full['Cabin'].apply(lambda x: 'N' if pd.isnull(x) else x[0])\n\nEDA_helper('Deck')",
      "execution_count": 107,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3bb11700a51ae4378f3c3256fee92a29869ddf0a"
      },
      "cell_type": "markdown",
      "source": "### Embarked"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2576d94da297078f6972839c8c01a5b0611329b4",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# fill the missing info with the most common value\nfor cabin in df_full['Embarked']:\n    df_full['Embarked'].fillna('S', inplace=True)\n\nEDA_helper('Embarked')",
      "execution_count": 129,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7762d5c717ac729f61990ecd68a5efe9a8491587"
      },
      "cell_type": "markdown",
      "source": "# Feature Engineering and Selection\n\n### New Feature: Family Survival\n\nThis is a feature from [S.Xu's](https://www.kaggle.com/shunjiangxu/blood-is-thicker-than-water-friendship-forever) and [Konstantin's kernel](https://www.kaggle.com/konstantinmasich/titanic-0-82-0-83/). "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fe470b633ed1f7b97422092a6fc638d0fce0f575",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# extract the last name from the 'Name' column (using the full_data)\nfor name in df_full['Name']:\n    df_full['Last Name'] = df_full['Name'].str.extract('([A-Za-z]+)\\,', expand=False)\n\nDEFAULT_SURVIVAL_VALUE = 0.5\ndf_full['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\n\nfor grp, grp_df in df_full[['Survived','Name', 'Last Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last Name', 'Fare']):\n    \n    if (len(grp_df) != 1):\n        # A Family group is found.\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                df_full.loc[df_full['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin==0.0):\n                df_full.loc[df_full['PassengerId'] == passID, 'Family_Survival'] = 0\n\nprint(\"Number of passengers with family survival information:\", \n      df_full.loc[df_full['Family_Survival']!=0.5].shape[0])",
      "execution_count": 109,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "81017340ca019d383b58edef0b652aef754f35d2",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "for _, grp_df in df_full.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    df_full.loc[df_full['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    df_full.loc[df_full['PassengerId'] == passID, 'Family_Survival'] = 0\n                        \nprint(\"Number of passenger with family/group survival information: \" \n      +str(df_full[df_full['Family_Survival']!=0.5].shape[0]))\n\ntrain_size = len(df_train)\n\n# Family_Survival in df_train and df_test:\ndf_train['Family_Survival'] = df_full['Family_Survival'][:train_size]\ndf_test['Family_Survival'] = df_full['Family_Survival'][train_size:]",
      "execution_count": 110,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1b44c2281975c1ad20930a2d2a739aaa5ccf8c7e"
      },
      "cell_type": "markdown",
      "source": "### Selecting Most Important Features"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4b2d91bd9b59097a9c05b738200f96a74fcf01c8",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# define a list of columns to work with going further\ncolumns_to_keep = ['Sex_code', 'Pclass', 'Fare_bin_code', 'Age_bin_code', 'Family Size_code', 'Family_Survival']\n\n# create new datafames with the desired columns\ntrain = df_train[columns_to_keep]\ntest = df_test[columns_to_keep]\n\n# save the target column for later use\ntrain_labels = df_train['Survived']\n\nprint('Train data shape: ', train.shape)\nprint('Test data shape: ', test.shape)",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c68e63aa26aa9d290010570b683867ea82627fff",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train.head()",
      "execution_count": 127,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2f2f8abdf793a5700e935fdd312c379cac27d4ca"
      },
      "cell_type": "markdown",
      "source": "# Modeling"
    },
    {
      "metadata": {
        "_uuid": "4e9957d16427c9ac3c88a0879af07f8d9647df27"
      },
      "cell_type": "markdown",
      "source": "### Gender Model\n\nThis simple model predicts that all female passengers survive while males don't."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7556008e06011b58d0c35bbfb53ef8942c111805",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# create simple predicition based on gender (women live, men die)\ngender_pred = df_test['Sex'].apply(lambda x: '1' if x=='female' else '0')\ngender_pred.value_counts()",
      "execution_count": 44,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c9fea479a8dcadfeffd459fdb997faf3f6fc8ff6"
      },
      "cell_type": "markdown",
      "source": "When submitted, this gender-only model will get a **score of 0.76555.**"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "cb5ac7109bcdf991053ef5364e762b8eda1b1f0a"
      },
      "cell_type": "markdown",
      "source": "### Random Forest Model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c47ba2f0dba976a4613b1ce5b605736353e86819",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# import libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\n\n# define the scaler instance\nscaler = StandardScaler()\n\n# fit on train data, transform both train and test data\ntrain = scaler.fit_transform(train)\ntest = scaler.transform(test)\n        \n# define the classifier instance\nclf = RandomForestClassifier(n_estimators=100, random_state = 42)\n\n# fit the classifier on the train data and previously saved train labels\nclf.fit(train, train_labels)\n\n# predict on test data\nrf_results = clf.predict(test)",
      "execution_count": 75,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "0c17ee21cd9ef1a65927d86dd75c0447fbd1abdc"
      },
      "cell_type": "code",
      "source": "# make a submission dataframe\nsubmit = df_test.loc[:, ['PassengerId']]\nsubmit.loc[:, 'Survived'] = rf_results\n\n# save the submission dataframe\nsubmit.to_csv('RF_submission.csv', index = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "20e6c98a65c671d02a84347dbd46696d952e27f2"
      },
      "cell_type": "markdown",
      "source": "When submitted, this random forest model will get a **score of 0.79425**"
    },
    {
      "metadata": {
        "_uuid": "4a6481e26cc9a1f8c7777e03f7291ec12119e227"
      },
      "cell_type": "markdown",
      "source": "### TPOT Model\n\nLast but not least, I will let the AutoML tool [TPOT](https://epistasislab.github.io/tpot/) run for 2 hours to automatically find a machine learning pipeline using genetic programming.\n\nInterestingly, TPOT also selected a random forest model which** scored 0.81818.**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0f5ec11c26b29bcdd7f2b74b7ec51d6c993cfa28",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# import TPOT\nfrom tpot import TPOTClassifier\n\n# create instance\npipeline_optimizer = TPOTClassifier(max_time_mins=120, n_jobs = -1, random_state=42, verbosity=2, cv=5)\n\n# fit TPOT on the train data\n# commented out after the run\n#pipeline_optimizer.fit(train, train_labels)\n\n# export optimized code\n# commented out after the run\n#pipeline_optimizer.export('tpot_titanic_pipeline.py')\n\n# import libraries\nfrom sklearn.pipeline import make_pipeline\n\n# create the pipeline from TPOT\n# original pipeline inluded a Binarizer and RBFSampler which scored only 0.78947 \nexported_pipeline = make_pipeline(\n    RandomForestClassifier(bootstrap=False, criterion=\"gini\", max_features=0.45, min_samples_leaf=14, min_samples_split=13, n_estimators=100)\n)\n\n# fit the pipeline on the train data\nexported_pipeline.fit(train, train_labels)\n\n# predict on the test data\nresults = exported_pipeline.predict(test)",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "0edd216f688b92b1741fec1372f26f373e15f6b3"
      },
      "cell_type": "code",
      "source": "# make a submission dataframe\nsubmit = df_test.loc[:, ['PassengerId']]\nsubmit.loc[:, 'Survived'] = results\n\n# save the submission dataframe\nsubmit.to_csv('TPOT_submission.csv', index = False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}